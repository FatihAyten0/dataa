# -*- coding: utf-8 -*-
"""new_transformer_ablation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VibqnT18k5Yqfx-TY28hMVdfw5P6riih
"""

# -*- coding: utf-8 -*-
"""latest_transformer.ipynb

Based on latest.ipynb â€” swapped MLP for Transformer with:
1. need_weights=False
2. pin_memory=True, num_workers=2
3. Pre-Norm (instead of Post-Norm)
4. GELU (instead of LeakyReLU)
5. dim_ff=1024, num_layers=3
6. Mean-pooling over AP tokens (instead of CLS-only)
7. Smaller regression head
"""

### EXPERIMENT SETTINGS ###
TRAIN_INPUT = "theo"   # "gp" or "theo"
N_SAMPLES_GP = 50
P_T = 0

import os, random

import numpy as np

from pathlib import Path
import pandas as pd
import shutil

def seed_everything(seed: int = 42, deterministic: bool = True):
    import os, random
    import numpy as np
    import torch

    os.environ["PYTHONHASHSEED"] = str(seed)

    random.seed(seed)
    np.random.seed(seed)

    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

    # Make CPU behavior more repeatable (optional but helpful)
    torch.set_num_threads(1)

    if deterministic:
        # CUDA determinism settings (only matter if you use GPU)
        os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

        # Avoid TF32 differences across GPUs
        torch.backends.cuda.matmul.allow_tf32 = False
        torch.backends.cudnn.allow_tf32 = False

        # Force deterministic algorithms (may throw if an op has no deterministic version)
        torch.use_deterministic_algorithms(True)

        # For PyTorch 2.x attention kernels, try to avoid non-deterministic fast paths
        try:
            torch.backends.cuda.enable_flash_sdp(False)
            torch.backends.cuda.enable_mem_efficient_sdp(False)
            torch.backends.cuda.enable_math_sdp(True)
        except Exception:
            pass

seed_everything(42, deterministic=True)

import glob, h5py, numpy as np

def load_h5_stack(pattern, dtype=np.float32, expected_cols=None, dset="/data"):
    files = sorted(glob.glob(pattern))
    if not files:
        raise FileNotFoundError(f"No files match: {pattern}")

    blocks = []
    for fn in files:
        with h5py.File(fn, "r") as f:
            A = f[dset][...]

        if A.ndim != 2:
            raise ValueError(f"{fn}: expected 2D, got {A.shape}")

        # Fix orientation if needed
        if expected_cols is not None:
            if A.shape[1] == expected_cols:
                pass  # OK: (N, expected_cols)
            elif A.shape[0] == expected_cols:
                A = A.T  # fix: (expected_cols, N) -> (N, expected_cols)
            else:
                raise ValueError(f"{fn}: cannot match expected_cols={expected_cols}, got {A.shape}")

        blocks.append(A.astype(dtype, copy=False))

    return np.concatenate(blocks, axis=0)

import numpy as np
import pandas as pd

def wrap_to_pi(x): # Wrap to (-pi, pi].
    return np.mod(x + np.pi, 2*np.pi) - np.pi

def make_ap_positions_3d(): # Generate 3D AP positions with fixed height h_ap=4.0
    ap_locs_xy = np.array([[5.00, 5.00],[4.5, 3.0],[2.0, 3.73],[6.5, 3.0],[3.0, 6.0],
        [5.0, 7.0],[8.0, 6.0],[8.0, 4.0],[7.0, 7.0],[1.2, 3.0],[2.3, 1.73],[7.5, 1.0],
        [8.0, 2.15],[1.3, 7.0],[0.5, 8.9],[8.0, 9.2],[7.0, 7.9],
    ], dtype=np.float64) # (17,2)

    h_ap = 4.0
    z = np.full((ap_locs_xy.shape[0], 1), h_ap, dtype=np.float64) # (17,1)
    ap_xyz = np.hstack([ap_locs_xy, z])  # (17,3)
    return ap_xyz, h_ap

def tokenize_measurements_arrays(
    phases,              # (N,M) absolute phases OR (N,M-1) differential
    ue_xyz,              # (N,3) labels
    fc_GHz=0.8,
    side_length_m=10.0,
    force_ap0_angles_and_proj_zero=True,
    include_cls_token=False,
    print_preview=True,
    num_preview_rows=3,
):
    """
    phases:
      - if shape (N,17): absolute phases for AP0..AP16 (this is your NEW MATLAB case)
      - if shape (N,16): treated as differential phases for AP1..AP16 with AP0=0 (old CSV case)

    Returns:
      X_tokens: (N, M, F)
      y:        (N, 3)
      ap_ids:   (M,)
      feature_names
    """
    ap_xyz, h_ap = make_ap_positions_3d()
    M = ap_xyz.shape[0]
    assert M == 17, "This code assumes ap_number=17."

    phases = np.asarray(phases)
    N = phases.shape[0]

    # --- NEW: accept absolute phases (N,M) OR old differential (N,M-1) ---
    if phases.shape[1] == M:
        phi = phases.astype(np.float64, copy=False)          # absolute phases for all APs
    elif phases.shape[1] == (M - 1):
        phi = np.zeros((N, M), dtype=np.float64)             # old style: AP0=0
        phi[:, 1:] = phases.astype(np.float64, copy=False)
    else:
        raise ValueError(f"phases must have shape (N,{M}) or (N,{M-1}), got {phases.shape}")

    y = np.asarray(ue_xyz, dtype=np.float64)
    if y.shape != (N, 3):
        raise ValueError(f"ue_xyz must have shape (N,3). Got {y.shape} for N={N}")

    # Signed wrap (still useful)
    phi_signed = wrap_to_pi(phi)

    # ---- Measurement features (same structure as before, just absolute now) ----
    sin_phi = np.sin(phi_signed)
    cos_phi = np.cos(phi_signed)

    c_m_per_ns = 0.299792458
    lam = c_m_per_ns / fc_GHz  # meters

    dd_mod_m = (lam / (2.0 * np.pi)) * phi_signed  # (N,M)

    # ---- Geometry relative to AP0 (unchanged) ----
    p = ap_xyz
    p0 = p[0:1, :]
    baseline = p - p0
    rel_mag = np.linalg.norm(baseline, axis=1)

    eps = 1e-12
    unit = baseline / (rel_mag[:, None] + eps)

    dx, dy, dz = baseline[:, 0], baseline[:, 1], baseline[:, 2]
    horiz = np.sqrt(dx**2 + dy**2) + eps

    az = np.arctan2(dy, dx)
    el = np.arctan2(dz, horiz)

    sin_az, cos_az = np.sin(az), np.cos(az)
    sin_el, cos_el = np.sin(el), np.cos(el)

    vproj = dd_mod_m[:, :, None] * unit[None, :, :]

    if force_ap0_angles_and_proj_zero:
        sin_az[0] = 0.0; cos_az[0] = 0.0
        sin_el[0] = 0.0; cos_el[0] = 0.0
        vproj[:, 0, :] = 0.0

    dd_scale = (lam / 2.0) + eps

    abs_pos = p[None, :, :]
    abs_pos_norm = np.empty((N, M, 3), dtype=np.float64)
    abs_pos_norm[:, :, 0] = abs_pos[0, :, 0] / side_length_m
    abs_pos_norm[:, :, 1] = abs_pos[0, :, 1] / side_length_m
    abs_pos_norm[:, :, 2] = abs_pos[0, :, 2] / h_ap

    rel_pos = baseline[None, :, :]
    rel_pos_norm = np.repeat(rel_pos, N, axis=0) / side_length_m
    rel_mag_norm = (rel_mag[None, :, None] / side_length_m)
    rel_mag_norm = np.repeat(rel_mag_norm, N, axis=0)

    dd_norm = (dd_mod_m / dd_scale)[:, :, None]
    vproj_norm = vproj / dd_scale

    angles = np.stack([sin_az, cos_az, sin_el, cos_el], axis=1)
    angles = angles[None, :, :]
    angles = np.repeat(angles, N, axis=0)

    is_ref = np.zeros((M,), dtype=np.float64)
    is_ref[0] = 1.0
    is_ref = np.repeat(is_ref[None, :, None], N, axis=0)

    X_tokens = np.concatenate([
        sin_phi[:, :, None],
        cos_phi[:, :, None]
    ], axis=2)

    feature_names = [
        "is_ref",
        "sin_phi", "cos_phi", "dd_norm",
        "abs_x_norm", "abs_y_norm", "abs_z_norm",
        "rel_dx_norm", "rel_dy_norm", "rel_dz_norm",
        "rel_mag_norm",
        "sin_az", "cos_az", "sin_el", "cos_el",
        "vproj_x_norm", "vproj_y_norm", "vproj_z_norm",
    ]

    ap_ids = np.arange(M, dtype=np.int32)

    if include_cls_token:
        cls = np.zeros((N, 1, X_tokens.shape[2]), dtype=np.float64)
        X_tokens = np.concatenate([cls, X_tokens], axis=1)

    if print_preview:
        print("Tokenized from arrays.")
        print("X shape:", X_tokens.shape, "(N, seq_len, F)")
        print("y shape:", y.shape)
        print("First sample, AP0 token:", X_tokens[0, 0, :])
        print("First sample, AP1 token:", X_tokens[0, 1, :])

    return X_tokens.astype(np.float32), y.astype(np.float32), ap_ids, feature_names



# ===== Cell 2: load MATLAB H5 shards + tokenize =====


train_dir = "train"
test_dir  = "test"

Xue_train = load_h5_stack(f"X_train_*.h5", dtype=np.float32, expected_cols=3)
Xue_test  = load_h5_stack(f"X_test_*.h5",  dtype=np.float32, expected_cols=3)


Ytheo_test  = load_h5_stack(f"Ytheo_test_*.h5",  dtype=np.float32, expected_cols=17)

if TRAIN_INPUT == "gp":
    Ygp_train = load_h5_stack(f"Ygp_train_*.h5", dtype=np.float32, expected_cols=17)
    phases_train = Ygp_train
else:
    Ytheo_train = load_h5_stack(f"Ytheo_train_*.h5", dtype=np.float32, expected_cols=17)
    phases_train = Ytheo_train

phases_test = Ytheo_test


# --- tokenize ---
X_train_tokens, y_train, ap_ids_np, feature_names = tokenize_measurements_arrays(
    phases=phases_train,
    ue_xyz=Xue_train,
    fc_GHz=0.8,
    side_length_m=10.0,
    force_ap0_angles_and_proj_zero=False,
    include_cls_token=False,
    print_preview=True
)

X_test_tokens, y_test, _, _ = tokenize_measurements_arrays(
    phases=phases_test,
    ue_xyz=Xue_test,
    fc_GHz=0.8,
    side_length_m=10.0,
    force_ap0_angles_and_proj_zero=False,
    include_cls_token=False,
    print_preview=False
)

print("Train tokens:", X_train_tokens.shape, "Train y:", y_train.shape)
print("Test  tokens:", X_test_tokens.shape,  "Test  y:", y_test.shape)
print("F features:", len(feature_names), feature_names)

# ===== Cell 1: imports =====
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ===== Cell 3: helpers for splits + normalization =====
def train_val_test_split(N, train_ratio=0.8, val_ratio=0.1, seed=42):
    rng = np.random.default_rng(seed)
    idx = np.arange(N)
    rng.shuffle(idx)
    n_train = int(train_ratio * N)
    n_val = int(val_ratio * N)
    train_idx = idx[:n_train]
    val_idx = idx[n_train:n_train+n_val]
    test_idx = idx[n_train+n_val:]
    return train_idx, val_idx, test_idx

class StandardScalerNP:
    """Simple mean/std scaler for numpy arrays."""
    def __init__(self):
        self.mean_ = None
        self.std_ = None

    def fit(self, x, eps=1e-12):
        self.mean_ = x.mean(axis=0, keepdims=True)
        self.std_ = x.std(axis=0, keepdims=True) + eps
        return self

    def transform(self, x):
        return (x - self.mean_) / self.std_

    def inverse_transform(self, x):
        return x * self.std_ + self.mean_


class TokenDataset(Dataset):
    def __init__(self, X, y):
        # store as float32 for speed on GPU
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, i):
        return self.X[i], self.y[i]

# ===== Cell 4: split + (optional) scale =====

X_np = X_train_tokens
y_np = y_train

N = X_np.shape[0]
train_idx, val_idx, _ = train_val_test_split(N, train_ratio=0.9, val_ratio=0.1, seed=42)

X_train, y_train = X_np[train_idx], y_np[train_idx]
X_val,   y_val   = X_np[val_idx],   y_np[val_idx]

# test comes from separate split
X_test,  y_test  = X_test_tokens, y_test


# ---- Feature scaling (optional, but often helpful) ----
# Your features are already mostly normalized. We'll keep this OFF by default.
scale_X = False
x_scaler = StandardScalerNP() if scale_X else None
if scale_X:
    # flatten tokens to compute per-feature stats
    X_train_flat = X_train.reshape(-1, X_train.shape[-1])
    x_scaler.fit(X_train_flat)
    def scale_tokens(X):
        Xf = X.reshape(-1, X.shape[-1])
        Xf = x_scaler.transform(Xf)
        return Xf.reshape(X.shape)
    X_train = scale_tokens(X_train)
    X_val   = scale_tokens(X_val)
    X_test  = scale_tokens(X_test)

# ---- Target scaling (recommended) ----
y_scaler = StandardScalerNP().fit(y_train)
y_train_s = y_scaler.transform(y_train)
y_val_s   = y_scaler.transform(y_val)
y_test_s  = y_scaler.transform(y_test)

train_ds = TokenDataset(X_train, y_train_s)
val_ds   = TokenDataset(X_val,   y_val_s)
test_ds  = TokenDataset(X_test,  y_test_s)

batch_size = 500
train_seed = 42  # (can be same as 42, but separating seeds is cleaner)
g = torch.Generator()
g.manual_seed(train_seed)
train_loader = DataLoader(
    train_ds,
    batch_size=batch_size,
    shuffle=True,
    drop_last=True,
    generator=g,
    num_workers=2,          # --- CHANGE 2: was 0 ---
    pin_memory=True,        # --- CHANGE 2: added ---
)
val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)    # --- CHANGE 2 ---
test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)    # --- CHANGE 2 ---

print("Train/Val/Test:", len(train_ds), len(val_ds), len(test_ds))

# ===== Cell 5: model =====
import torch
import torch.nn as nn


class TransformerEncoderBlock(nn.Module):
    """
    Pre-Norm Transformer encoder block.                          # --- CHANGE 3: Pre-Norm ---
    Shapes:
      x: (B, S, D)
    """
    def __init__(self, d_model, nhead, dim_ff, dropout):
        super().__init__()

        self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(d_model)
        self.drop1 = nn.Dropout(dropout)

        # 2-layer FFN with GELU                                  # --- CHANGE 4: GELU ---
        self.ff = nn.Sequential(
            nn.Linear(d_model, dim_ff),
            nn.GELU(),                                            # --- CHANGE 4: was LeakyReLU ---
            nn.Dropout(dropout),
            nn.Linear(dim_ff, d_model),
            nn.Dropout(dropout),
        )
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # --- CHANGE 3: Pre-Norm (norm BEFORE sublayer, residual on raw x) ---
        x_norm = self.norm1(x)
        attn_out, _ = self.mha(                                   # --- CHANGE 1: need_weights=False ---
            x_norm, x_norm, x_norm,
            need_weights=False,                                   # --- CHANGE 1: was True ---
        )
        x = x + self.drop1(attn_out)

        x_norm = self.norm2(x)
        x = x + self.ff(x_norm)
        return x


class UETransformerRegressor(nn.Module):
    def __init__(
        self,
        num_aps=17,
        in_features=18,
        d_model=256,
        nhead=4,
        num_layers=3,       # --- CHANGE 5: was 5 ---
        dim_ff=1024,        # --- CHANGE 5: was 256 ---
        dropout=0.01,
        two_heads=False,
    ):
        super().__init__()
        self.num_aps = num_aps
        self.in_features = in_features
        self.d_model = d_model
        self.two_heads = two_heads

        # Project input features (F) -> model dimension (D)
        self.in_proj = nn.Linear(in_features, d_model)

        # AP-ID embedding (no CLS needed for mean-pooling, but keep for compatibility)
        self.ap_embed = nn.Embedding(num_aps, d_model)

        self.drop = nn.Dropout(dropout)

        self.layers = nn.ModuleList([
            TransformerEncoderBlock(d_model, nhead, dim_ff, dropout)
            for _ in range(num_layers)
        ])

        # Final LayerNorm after all blocks (standard for Pre-Norm)
        self.final_norm = nn.LayerNorm(d_model)

        # --- CHANGE 7: smaller regression head ---
        if two_heads:
            self.head_xy = nn.Sequential(
                nn.LayerNorm(d_model),
                nn.Linear(d_model, d_model),
                nn.GELU(),                                        # --- CHANGE 4 ---
                nn.Linear(d_model, 2)
            )
            self.head_z = nn.Sequential(
                nn.LayerNorm(d_model),
                nn.Linear(d_model, d_model),
                nn.GELU(),                                        # --- CHANGE 4 ---
                nn.Linear(d_model, 1)
            )
        else:
            self.head = nn.Sequential(
                nn.LayerNorm(d_model),
                nn.Linear(d_model, 128),                          # --- CHANGE 7: was 512 ---
                nn.GELU(),                                        # --- CHANGE 4 ---
                nn.Linear(128, 3)                                 # --- CHANGE 7: was 256->3 ---
            )

    def forward(self, x):
        """
        x: (B, M, F) where M=num_aps, F=number of features
        """
        B, M, F = x.shape

        # Project features to D
        t = self.in_proj(x)  # (B, M, D)

        # Add AP-ID embeddings: [0, 1, 2, ..., M-1]
        ap_ids = torch.arange(M, device=x.device).unsqueeze(0).expand(B, -1)  # (B, M)
        t = t + self.ap_embed(ap_ids)

        t = self.drop(t)

        for layer in self.layers:
            t = layer(t)

        # Final norm (needed for Pre-Norm architecture)
        t = self.final_norm(t)

        # --- CHANGE 6: mean-pool over all AP tokens instead of CLS ---
        pooled = t.mean(dim=1)  # (B, D)

        if self.two_heads:
            xy = self.head_xy(pooled)
            z  = self.head_z(pooled)
            out = torch.cat([xy, z], dim=1)  # (B,3)
        else:
            out = self.head(pooled)

        return out

# ===== Cell 6 (REPLACE with this): training utilities + tqdm =====
from tqdm import tqdm
import time

def make_scheduler(optimizer, scheduler_name, num_epochs, steps_per_epoch, warmup_ratio=0.05):
    scheduler_name = scheduler_name.lower()
    total_steps = num_epochs * steps_per_epoch
    warmup_steps = int(warmup_ratio * total_steps)

    if scheduler_name == "none":
        return None

    if scheduler_name == "onecycle":
        max_lr = optimizer.param_groups[0]["lr"]
        return torch.optim.lr_scheduler.OneCycleLR(
            optimizer,
            max_lr=max_lr,
            total_steps=total_steps,
            pct_start=warmup_ratio,
            anneal_strategy="cos",
            cycle_momentum=False
        )

    if scheduler_name == "cosine_warmup":
        def lr_lambda(step):
            if step < warmup_steps:
                return (step + 1) / max(1, warmup_steps)
            progress = (step - warmup_steps) / max(1, (total_steps - warmup_steps))
            return 0.5 * (1.0 + np.cos(np.pi * progress))
        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

    raise ValueError(f"Unknown scheduler_name='{scheduler_name}'")


@torch.no_grad()
def evaluate(model, loader, criterion):
    model.eval()
    losses = []
    for Xb, yb in loader:
        Xb, yb = Xb.to(device), yb.to(device)
        pred = model(Xb)
        loss = criterion(pred, yb)
        losses.append(loss.item())
    return float(np.mean(losses))


def train_model(
    X_shape,  # (N, M, F)
    train_loader,
    val_loader,
    test_loader=None,
    eval_test_every=1,
    num_epochs=30,
    d_model=128,
    nhead=8,
    num_layers=4,
    dim_ff=256,
    dropout=0.1,
    two_heads=False,
    lr=3e-4,
    weight_decay=1e-2,
    scheduler_name="cosine_warmup",
    warmup_ratio=0.05,
    grad_clip=1.0,
):
    _, M, F = X_shape

    train_losses, val_losses, test_losses = [], [], []
    lrs = []

    global_step = 0

    model = UETransformerRegressor(
        num_aps=M,
        in_features=F,
        d_model=d_model,
        nhead=nhead,
        num_layers=num_layers,
        dim_ff=dim_ff,
        dropout=dropout,
        two_heads=two_heads
    ).to(device)

    model = torch.compile(model)                # <-- NEW: torch.compile

    criterion = nn.L1Loss()
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    steps_per_epoch = len(train_loader)
    scheduler = make_scheduler(optimizer, scheduler_name, num_epochs, steps_per_epoch, warmup_ratio)

    train_losses, val_losses = [], []
    global_step = 0

    scaler = torch.amp.GradScaler('cuda')       # <-- NEW: AMP scaler

    t0_all = time.perf_counter()

    for epoch in range(1, num_epochs + 1):
        t0 = time.perf_counter()
        model.train()
        epoch_losses = []

        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{num_epochs}", leave=False)

        for Xb, yb in pbar:
            Xb, yb = Xb.to(device), yb.to(device)

            optimizer.zero_grad()

            with torch.amp.autocast('cuda'):     # <-- NEW: AMP forward
                pred = model(Xb)
                loss = criterion(pred, yb)

            scaler.scale(loss).backward()        # <-- NEW: scaled backward

            if grad_clip is not None:
                scaler.unscale_(optimizer)        # <-- NEW: unscale before clipping
                nn.utils.clip_grad_norm_(model.parameters(), grad_clip)

            scaler.step(optimizer)               # <-- NEW: scaler steps optimizer
            scaler.update()                      # <-- NEW: update scale factor

            if scheduler is not None:
                scheduler.step()

            epoch_losses.append(loss.item())
            global_step += 1

            cur_lr = optimizer.param_groups[0]["lr"]
            lrs.append(cur_lr)
            pbar.set_postfix(
                loss=f"{loss.item():.3e}",
                avg=f"{np.mean(epoch_losses):.3e}",
                lr=f"{cur_lr:.2e}"
            )

        tr = float(np.mean(epoch_losses))
        va = evaluate(model, val_loader, criterion)

        te = None
        if (test_loader is not None) and (eval_test_every is not None) and (eval_test_every > 0) and (epoch % eval_test_every == 0):
            te = evaluate(model, test_loader, criterion)

        train_losses.append(tr)
        val_losses.append(va)
        test_losses.append(te if te is not None else np.nan)

        epoch_time = time.perf_counter() - t0
        total_time = time.perf_counter() - t0_all
        cur_lr = optimizer.param_groups[0]["lr"]

        msg = (
            f"Epoch {epoch:03d}/{num_epochs} | "
            f"time {epoch_time:6.2f}s | total {total_time/60:6.2f} min | "
            f"lr {cur_lr:.2e} | train {tr:.6e} | val {va:.6e}"
        )
        if te is not None:
            msg += f" | test {te:.6e}"
        print(msg)

    return model, train_losses, val_losses, test_losses, lrs

# ===== Cell 7: run training =====
import time

seed_everything(train_seed, deterministic=True)

t0 = time.perf_counter()

model, tr_losses, va_losses, te_losses, lrs = train_model(
    X_shape=X_train.shape,         # (N, M, F)
    train_loader=train_loader,
    val_loader=val_loader,
    test_loader=test_loader,     # <-- NEW
    eval_test_every=1,           # <-- NEW (or 5 if you want faster)
    num_epochs=120,

    d_model=256,
    nhead=4,
    num_layers=3,       # --- CHANGE 5: was 4/5 ---
    dim_ff=1024,        # --- CHANGE 5: was 256 ---
    dropout=0.01,
    two_heads=False,

    lr=10e-4,
    weight_decay=0.5e-6,
    scheduler_name="onecycle",  # "none", "onecycle", "cosine_warmup"
    warmup_ratio=0.05,
    grad_clip=1.0
)




t1 = time.perf_counter()
print(f"Total training runtime: {t1 - t0:.2f} seconds ({(t1 - t0)/60:.2f} minutes)")

# ===== Plot learning rate vs epochs =====
plt.figure()
plt.plot(lrs)
plt.xlabel("epoch")
plt.ylabel("learning rate")
plt.grid(True)
plt.show()

# ===== Cell 8: plot train/val loss =====
plt.figure()
plt.plot(tr_losses, label="train")
plt.plot(va_losses, label="val")
plt.plot(te_losses, label="test")   # <-- NEW
plt.yscale("log")
plt.xlabel("epoch")
plt.ylabel("MAE (scaled target space)")
plt.legend()
plt.grid(True)
plt.show()

# ===== Cell 9: test metrics in real units =====
@torch.no_grad()
def predict_all(model, loader):
    model.eval()
    preds = []
    trues = []
    for Xb, yb in loader:
        Xb = Xb.to(device)
        pred = model(Xb).cpu().numpy()
        preds.append(pred)
        trues.append(yb.numpy())
    return np.vstack(preds), np.vstack(trues)

pred_s, true_s = predict_all(model, test_loader)

# invert scaling back to meters
pred_m = y_scaler.inverse_transform(pred_s)
true_m = y_scaler.inverse_transform(true_s)

err = pred_m - true_m
rmse_xyz = np.sqrt(np.mean(err**2, axis=0))
rmse_3d = np.sqrt(np.mean(np.sum(err**2, axis=1)))
mean_3d_dist = np.mean(np.linalg.norm(err, axis=1))


print("RMSE x,y,z (m):", rmse_xyz)
print("RMSE 3D (m):", rmse_3d)
print("RMSE 3D (mm):", rmse_3d * 1000.0)
print("Mean 3D distance error (m):", mean_3d_dist)
print("Mean 3D distance error (mm):", mean_3d_dist * 1000.0)

# Save err

from pathlib import Path
from scipy.io import savemat

# choose any name you want
if TRAIN_INPUT == 'xxx':
  tag = f"err_{TRAIN_INPUT}_{N_SAMPLES_GP}_samples_{P_T}dbm"
if TRAIN_INPUT == 'theo':
  tag = f"err_{TRAIN_INPUT}_{P_T}dbm"

out_dir = Path("artifacts")
out_dir.mkdir(parents=True, exist_ok=True)

fname = out_dir / f"{tag}.mat"

savemat(fname, {
    "err": err,                     # (N,3)
    "pred_m": pred_m,
    "true_m": true_m,
    "rmse_xyz": rmse_xyz,
    "rmse_3d": rmse_3d,
    "mean_3d_dist": mean_3d_dist
})

print("Saved:", fname)

import numpy as np
import matplotlib.pyplot as plt

# 3D distance error per sample (meters)
d = np.linalg.norm(err, axis=1).astype(float)

# (optional) avoid log(0) issues if any perfect predictions exist
d = np.clip(d, np.finfo(float).tiny, None)

# empirical CDF
x = np.sort(d)
y = np.arange(1, len(x) + 1) / len(x)

plt.figure()
plt.plot(x, y)
plt.xscale('log')
plt.xlabel('3D distance error (m)')
plt.ylabel('CDF')
plt.grid(True, which='both')
plt.savefig("cdf_3d_error_logx.png", dpi=300, bbox_inches="tight")  # <--- save

plt.show()